# ============================================================
# NOTEBOOK 04 — TRANSFORMER EMBEDDING EXTRACTION (ONE CELL)
# Outputs embeddings for multimodal classifier
# ============================================================

# --- minimal deps (and avoid HF datasets/pyarrow problems) ---
!pip -q install -U transformers scikit-learn
!pip -q uninstall -y datasets pyarrow

import os, json, time, sys, platform
import numpy as np
import pandas as pd
import torch

from transformers import AutoTokenizer, AutoModel

print("Python:", sys.version)
print("Platform:", platform.platform())
print("Torch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------------------------------------------
# 1) Load splits from Notebook 02 outputs
# ------------------------------------------------------------
DATA_DIR = "/content/data"
TRAIN_PATH = os.path.join(DATA_DIR, "train.csv")
VAL_PATH   = os.path.join(DATA_DIR, "val.csv")
TEST_PATH  = os.path.join(DATA_DIR, "test.csv")

for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:
    if not os.path.exists(p):
        raise FileNotFoundError(f"Missing split file: {p}. Run Notebook 02 first to generate train/val/test CSVs.")

train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

print("Loaded splits:")
print("Train:", train_df.shape, "Cols:", list(train_df.columns))
print("Val:  ", val_df.shape,   "Cols:", list(val_df.columns))
print("Test: ", test_df.shape,  "Cols:", list(test_df.columns))

# ------------------------------------------------------------
# 2) Standardise schema to text + label
# ------------------------------------------------------------
def standardise(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # already standard
    if "text" in df.columns and "label" in df.columns:
        out = df[["text", "label"]].copy()
        out["text"] = out["text"].astype(str)
        out["label"] = out["label"].astype(int)
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    # Notebook 02 format
    if "Category" in df.columns and "Message" in df.columns:
        label_map = {"ham": 0, "spam": 1}
        df["Category"] = df["Category"].astype(str).str.strip().str.lower()
        unknown = set(df["Category"].unique()) - set(label_map.keys())
        if unknown:
            raise ValueError(f"Unknown labels found in Category: {unknown}")

        out = pd.DataFrame({
            "text": df["Message"].astype(str),
            "label": df["Category"].map(label_map).astype(int)
        })
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    raise ValueError(f"Unexpected columns in split CSV: {list(df.columns)}")

train_df = standardise(train_df)
val_df   = standardise(val_df)
test_df  = standardise(test_df)

print("\nStandardised splits -> columns:", train_df.columns.tolist())
print("Label counts (train):", train_df["label"].value_counts().to_dict())

# ------------------------------------------------------------
# 3) Load tokenizer + encoder model
#    Prefer the fine-tuned baseline directory if available
# ------------------------------------------------------------
BASELINE_DIR = "/content/baseline_distilbert_saved"
MODEL_NAME = BASELINE_DIR if os.path.exists(BASELINE_DIR) else "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
encoder = AutoModel.from_pretrained(MODEL_NAME)   # encoder-only (no classification head)
encoder.to(device)
encoder.eval()

print("\nLoaded encoder from:", MODEL_NAME)

# ------------------------------------------------------------
# 4) Embedding extraction helpers
# ------------------------------------------------------------
MAX_LENGTH = 256
BATCH_SIZE = 64 if torch.cuda.is_available() else 16

def masked_mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    # last_hidden_state: [B, T, H]
    # attention_mask:   [B, T]
    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # [B, T, 1]
    summed = (last_hidden_state * mask).sum(dim=1)                  # [B, H]
    counts = mask.sum(dim=1).clamp(min=1e-9)                        # [B, 1]
    return summed / counts

@torch.no_grad()
def extract_embeddings(texts: list[str]) -> tuple[np.ndarray, np.ndarray]:
    """
    Returns:
      cls_emb:  [N, H]
      mean_emb: [N, H]
    """
    cls_out = []
    mean_out = []

    for i in range(0, len(texts), BATCH_SIZE):
        batch_texts = texts[i:i+BATCH_SIZE]
        enc = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )
        enc = {k: v.to(device) for k, v in enc.items()}

        outputs = encoder(**enc)
        last_hidden = outputs.last_hidden_state  # [B, T, H]

        cls_emb = last_hidden[:, 0, :]  # [B, H] (CLS token)
        mean_emb = masked_mean_pool(last_hidden, enc["attention_mask"])  # [B, H]

        cls_out.append(cls_emb.detach().cpu().numpy())
        mean_out.append(mean_emb.detach().cpu().numpy())

    cls_out = np.concatenate(cls_out, axis=0)
    mean_out = np.concatenate(mean_out, axis=0)
    return cls_out, mean_out

# ------------------------------------------------------------
# 5) Extract embeddings for each split
# ------------------------------------------------------------
def run_split(name: str, df: pd.DataFrame):
    texts = df["text"].astype(str).tolist()
    labels = df["label"].astype(int).to_numpy()

    t0 = time.perf_counter()
    cls_emb, mean_emb = extract_embeddings(texts)
    t1 = time.perf_counter()

    print(f"\n{name.upper()} extracted:")
    print(" - cls:", cls_emb.shape, "mean:", mean_emb.shape, "labels:", labels.shape)
    print(f" - time: {t1 - t0:.2f}s | per-sample: {(t1 - t0)/len(texts):.6f}s")

    return cls_emb, mean_emb, labels

train_cls, train_mean, train_y = run_split("train", train_df)
val_cls, val_mean, val_y       = run_split("val", val_df)
test_cls, test_mean, test_y    = run_split("test", test_df)

# ------------------------------------------------------------
# 6) Save outputs
# ------------------------------------------------------------
OUT_DIR = "/content/embeddings_distilbert"
os.makedirs(OUT_DIR, exist_ok=True)

np.save(os.path.join(OUT_DIR, "train_cls.npy"), train_cls)
np.save(os.path.join(OUT_DIR, "val_cls.npy"), val_cls)
np.save(os.path.join(OUT_DIR, "test_cls.npy"), test_cls)

np.save(os.path.join(OUT_DIR, "train_mean.npy"), train_mean)
np.save(os.path.join(OUT_DIR, "val_mean.npy"), val_mean)
np.save(os.path.join(OUT_DIR, "test_mean.npy"), test_mean)

np.save(os.path.join(OUT_DIR, "train_labels.npy"), train_y)
np.save(os.path.join(OUT_DIR, "val_labels.npy"), val_y)
np.save(os.path.join(OUT_DIR, "test_labels.npy"), test_y)

meta = {
    "model_loaded_from": MODEL_NAME,
    "max_length": MAX_LENGTH,
    "batch_size": BATCH_SIZE,
    "embedding_types": ["cls", "mean"],
    "splits": {
        "train": {"n": int(len(train_y)), "dim": int(train_cls.shape[1])},
        "val": {"n": int(len(val_y)), "dim": int(val_cls.shape[1])},
        "test": {"n": int(len(test_y)), "dim": int(test_cls.shape[1])},
    },
    "label_encoding": {"ham": 0, "spam": 1}
}

with open(os.path.join(OUT_DIR, "embedding_meta.json"), "w") as f:
    json.dump(meta, f, indent=2)

print("\n✅ Embeddings saved to:", OUT_DIR)
print("Files:", sorted(os.listdir(OUT_DIR))[:20], "...")
print("\nReady for multimodal classifier (Notebook 05/06).")

