# ============================================================
# NOTEBOOK 03 — BASELINE DISTILBERT (TEXT-ONLY)
# ONE-CELL, CLEAN, GITHUB-READY (Transformers new API)
# ============================================================

# -----------------------------
# ENV SETUP (SAFE)
# -----------------------------
!pip -q install -U transformers evaluate scikit-learn accelerate
!pip -q uninstall -y datasets pyarrow

# -----------------------------
# RUNTIME CHECK
# -----------------------------
import sys, platform, os, time, random, json
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset

print("Python:", sys.version)
print("Platform:", platform.platform())
print("Torch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

# -----------------------------
# REPRODUCIBILITY
# -----------------------------
from transformers import set_seed
SEED = 42
set_seed(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
print("Seed fixed:", SEED)

# -----------------------------
# LOAD DATA (FROM NOTEBOOK 02)
# -----------------------------
DATA_DIR = "/content/data"
train_df = pd.read_csv(os.path.join(DATA_DIR, "train.csv"))
val_df   = pd.read_csv(os.path.join(DATA_DIR, "val.csv"))
test_df  = pd.read_csv(os.path.join(DATA_DIR, "test.csv"))

print("Train:", train_df.shape)
print("Val:", val_df.shape)
print("Test:", test_df.shape)
print("Train columns:", list(train_df.columns))

# -----------------------------
# STANDARDISE SCHEMA
# -----------------------------
def standardise(df):
    df = df.copy()
    if "text" in df.columns and "label" in df.columns:
        out = df[["text", "label"]].copy()
        out["text"] = out["text"].astype(str)
        out["label"] = out["label"].astype(int)
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    # Notebook 02 format expected: Category, Message
    if "Category" in df.columns and "Message" in df.columns:
        label_map = {"ham": 0, "spam": 1}
        df["Category"] = df["Category"].astype(str).str.lower().str.strip()
        unknown = set(df["Category"].unique()) - set(label_map.keys())
        if unknown:
            raise ValueError(f"Unknown Category labels found: {unknown}")

        out = pd.DataFrame({
            "text": df["Message"].astype(str),
            "label": df["Category"].map(label_map).astype(int)
        })
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    raise ValueError(f"Unexpected columns found: {list(df.columns)}")

train_df = standardise(train_df)
val_df   = standardise(val_df)
test_df  = standardise(test_df)

print("Schema:", train_df.columns.tolist())

# -----------------------------
# LABEL DISTRIBUTION
# -----------------------------
print("\nLabel distribution:")
print("Train:\n", train_df["label"].value_counts())
print("Val:\n", val_df["label"].value_counts())
print("Test:\n", test_df["label"].value_counts())

# -----------------------------
# TOKENIZER
# -----------------------------
from transformers import AutoTokenizer
MODEL_NAME = "distilbert-base-uncased"
MAX_LENGTH = 256
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
print("Tokenizer loaded:", MODEL_NAME)

# -----------------------------
# TORCH DATASET
# -----------------------------
class EmailDataset(Dataset):
    def __init__(self, df):
        self.texts = df["text"].astype(str).tolist()
        self.labels = df["label"].astype(int).tolist()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        enc = tokenizer(self.texts[idx], truncation=True, max_length=MAX_LENGTH)
        enc["labels"] = self.labels[idx]
        return enc

train_ds = EmailDataset(train_df)
val_ds   = EmailDataset(val_df)
test_ds  = EmailDataset(test_df)

from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer)

# -----------------------------
# MODEL
# -----------------------------
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

# -----------------------------
# METRICS
# -----------------------------
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()
    preds = np.argmax(probs, axis=1)

    acc = accuracy_score(labels, preds)
    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average="binary", zero_division=0)
    try:
        roc = roc_auc_score(labels, probs[:, 1])
    except Exception:
        roc = float("nan")

    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1, "roc_auc": roc}

# -----------------------------
# TRAINING ARGUMENTS (NEW API)
# -----------------------------
from transformers import TrainingArguments, Trainer

OUTPUT_DIR = "/content/outputs_baseline_distilbert"

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    seed=SEED,

    eval_strategy="epoch",      # ✅ new name
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,

    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,

    logging_strategy="steps",
    logging_steps=50,

    report_to="none",
    fp16=torch.cuda.is_available(),
    save_total_limit=2
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# -----------------------------
# TRAIN
# -----------------------------
train_result = trainer.train()
print(train_result)

# -----------------------------
# EVALUATION
# -----------------------------
val_metrics = trainer.evaluate(val_ds)
test_metrics = trainer.evaluate(test_ds)

print("\nValidation metrics:", val_metrics)
print("Test metrics:", test_metrics)

# -----------------------------
# CONFUSION MATRIX (TEST)
# -----------------------------
pred = trainer.predict(test_ds)
logits = pred.predictions
labels = pred.label_ids

probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()
preds = np.argmax(probs, axis=1)

cm = confusion_matrix(labels, preds)
tn, fp, fn, tp = cm.ravel()

print("\nConfusion Matrix:")
print(cm)
print(f"TN={tn}, FP={fp}, FN={fn}, TP={tp}")

# -----------------------------
# LATENCY
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

texts = test_df["text"].astype(str).tolist()[:300]
with torch.no_grad():
    warm = tokenizer(texts[:8], return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
    warm = {k: v.to(device) for k, v in warm.items()}
    _ = model(**warm)

start = time.perf_counter()
with torch.no_grad():
    batch = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=MAX_LENGTH)
    batch = {k: v.to(device) for k, v in batch.items()}
    _ = model(**batch)
end = time.perf_counter()

latency = (end - start) / len(texts)
print("Latency per sample (s):", latency)

# -----------------------------
# SAVE ARTIFACTS
# -----------------------------
SAVE_DIR = "/content/baseline_distilbert_saved"
trainer.save_model(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)

results = {
    "seed": SEED,
    "model": MODEL_NAME,
    "max_length": MAX_LENGTH,
    "val_metrics": val_metrics,
    "test_metrics": test_metrics,
    "confusion_matrix": cm.tolist(),
    "tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp),
    "latency_per_sample_seconds": float(latency)
}

os.makedirs(OUTPUT_DIR, exist_ok=True)

with open(os.path.join(OUTPUT_DIR, "baseline_results.json"), "w") as f:
    json.dump(results, f, indent=2)

pd.DataFrame([{
    **{f"val_{k}": v for k, v in val_metrics.items()},
    **{f"test_{k}": v for k, v in test_metrics.items()},
    "tn": tn, "fp": fp, "fn": fn, "tp": tp,
    "latency_per_sample_seconds": float(latency)
}]).to_csv(os.path.join(OUTPUT_DIR, "baseline_results_summary.csv"), index=False)

print("\n✅ Baseline complete. Ready for Notebook 04.")
print("Saved model:", SAVE_DIR)
print("Saved results:", OUTPUT_DIR)
