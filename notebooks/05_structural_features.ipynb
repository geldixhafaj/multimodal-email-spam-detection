# ============================================================
# NOTEBOOK 05 — STRUCTURAL FEATURE EXTRACTION (ONE CELL)
# Outputs structural features for multimodal classifier
# ============================================================

# --- minimal deps (avoid HF datasets/pyarrow problems) ---
!pip -q install -U scikit-learn numpy pandas
!pip -q uninstall -y datasets pyarrow

import os, re, json, sys, platform
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler

print("Python:", sys.version)
print("Platform:", platform.platform())

# ------------------------------------------------------------
# 1) Load splits from Notebook 02 outputs
# ------------------------------------------------------------
DATA_DIR = "/content/data"
TRAIN_PATH = os.path.join(DATA_DIR, "train.csv")
VAL_PATH   = os.path.join(DATA_DIR, "val.csv")
TEST_PATH  = os.path.join(DATA_DIR, "test.csv")

for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:
    if not os.path.exists(p):
        raise FileNotFoundError(f"Missing split file: {p}. Run Notebook 02 first to generate train/val/test CSVs.")

train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

print("Loaded splits:")
print("Train:", train_df.shape, "Cols:", list(train_df.columns))
print("Val:  ", val_df.shape,   "Cols:", list(val_df.columns))
print("Test: ", test_df.shape,  "Cols:", list(test_df.columns))

# ------------------------------------------------------------
# 2) Standardise schema to text + label (0 ham, 1 spam)
# ------------------------------------------------------------
def standardise(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    if "text" in df.columns and "label" in df.columns:
        out = df[["text", "label"]].copy()
        out["text"] = out["text"].astype(str)
        out["label"] = out["label"].astype(int)
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    if "Category" in df.columns and "Message" in df.columns:
        label_map = {"ham": 0, "spam": 1}
        df["Category"] = df["Category"].astype(str).str.strip().str.lower()
        unknown = set(df["Category"].unique()) - set(label_map.keys())
        if unknown:
            raise ValueError(f"Unknown labels found in Category: {unknown}")

        out = pd.DataFrame({
            "text": df["Message"].astype(str),
            "label": df["Category"].map(label_map).astype(int)
        })
        out = out[out["text"].str.strip().astype(bool)].reset_index(drop=True)
        return out

    raise ValueError(f"Unexpected columns in split CSV: {list(df.columns)}")

train_df = standardise(train_df)
val_df   = standardise(val_df)
test_df  = standardise(test_df)

print("\nStandardised -> columns:", train_df.columns.tolist())
print("Train label counts:", train_df["label"].value_counts().to_dict())

# ------------------------------------------------------------
# 3) Structural feature functions
# ------------------------------------------------------------
url_regex = re.compile(r"(http[s]?://\S+|www\.\S+)", re.IGNORECASE)
html_regex = re.compile(r"<[^>]+>")  # rough HTML tag detection
currency_regex = re.compile(r"[$£€¥₹]", re.UNICODE)

def safe_text(x) -> str:
    return "" if x is None else str(x)

def uppercase_ratio(text: str) -> float:
    text = safe_text(text)
    if len(text) == 0:
        return 0.0
    upper = sum(1 for c in text if c.isupper())
    return upper / max(len(text), 1)

def digit_ratio(text: str) -> float:
    text = safe_text(text)
    if len(text) == 0:
        return 0.0
    digits = sum(1 for c in text if c.isdigit())
    return digits / max(len(text), 1)

def punctuation_ratio(text: str) -> float:
    text = safe_text(text)
    if len(text) == 0:
        return 0.0
    punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()))
    return punct / max(len(text), 1)

def extract_features(df: pd.DataFrame) -> np.ndarray:
    texts = df["text"].astype(str).tolist()

    char_len = np.array([len(t) for t in texts], dtype=np.float32)
    word_len = np.array([len(t.split()) for t in texts], dtype=np.float32)

    url_count = np.array([len(url_regex.findall(t)) for t in texts], dtype=np.float32)
    has_url = (url_count > 0).astype(np.float32)

    digits = np.array([sum(1 for c in t if c.isdigit()) for t in texts], dtype=np.float32)
    digit_rat = np.array([digit_ratio(t) for t in texts], dtype=np.float32)

    upper_rat = np.array([uppercase_ratio(t) for t in texts], dtype=np.float32)

    currency_count = np.array([len(currency_regex.findall(t)) for t in texts], dtype=np.float32)
    has_currency = (currency_count > 0).astype(np.float32)

    punct_rat = np.array([punctuation_ratio(t) for t in texts], dtype=np.float32)

    exclam_count = np.array([t.count("!") for t in texts], dtype=np.float32)
    question_count = np.array([t.count("?") for t in texts], dtype=np.float32)

    has_html = np.array([1.0 if html_regex.search(t) else 0.0 for t in texts], dtype=np.float32)

    # Stack features in a fixed order
    X = np.stack([
        char_len,
        word_len,
        url_count,
        has_url,
        digits,
        digit_rat,
        upper_rat,
        currency_count,
        has_currency,
        punct_rat,
        exclam_count,
        question_count,
        has_html
    ], axis=1)

    return X

feature_names = [
    "char_len",
    "word_len",
    "url_count",
    "has_url",
    "digit_count",
    "digit_ratio",
    "uppercase_ratio",
    "currency_count",
    "has_currency",
    "punctuation_ratio",
    "exclam_count",
    "question_count",
    "has_html"
]

# ------------------------------------------------------------
# 4) Extract raw features
# ------------------------------------------------------------
X_train_raw = extract_features(train_df)
X_val_raw   = extract_features(val_df)
X_test_raw  = extract_features(test_df)

y_train = train_df["label"].astype(int).to_numpy()
y_val   = val_df["label"].astype(int).to_numpy()
y_test  = test_df["label"].astype(int).to_numpy()

print("\nRaw structural features:")
print("X_train:", X_train_raw.shape, "| y_train:", y_train.shape)
print("X_val:  ", X_val_raw.shape,   "| y_val:  ", y_val.shape)
print("X_test: ", X_test_raw.shape,  "| y_test: ", y_test.shape)

# ------------------------------------------------------------
# 5) Scale numeric features (fit on train only)
# ------------------------------------------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train_raw).astype(np.float32)
X_val   = scaler.transform(X_val_raw).astype(np.float32)
X_test  = scaler.transform(X_test_raw).astype(np.float32)

print("\nScaled structural features (mean ~0, std ~1 on train):")
print("X_train:", X_train.shape, "dtype:", X_train.dtype)

# ------------------------------------------------------------
# 6) Save outputs
# ------------------------------------------------------------
OUT_DIR = "/content/struct_features"
os.makedirs(OUT_DIR, exist_ok=True)

np.save(os.path.join(OUT_DIR, "train_struct.npy"), X_train)
np.save(os.path.join(OUT_DIR, "val_struct.npy"), X_val)
np.save(os.path.join(OUT_DIR, "test_struct.npy"), X_test)

np.save(os.path.join(OUT_DIR, "train_labels.npy"), y_train)
np.save(os.path.join(OUT_DIR, "val_labels.npy"), y_val)
np.save(os.path.join(OUT_DIR, "test_labels.npy"), y_test)

with open(os.path.join(OUT_DIR, "struct_feature_names.json"), "w") as f:
    json.dump(feature_names, f, indent=2)

meta = {
    "source_splits": {
        "train": TRAIN_PATH,
        "val": VAL_PATH,
        "test": TEST_PATH
    },
    "n_features": int(X_train.shape[1]),
    "feature_names_file": "struct_feature_names.json",
    "scaling": "StandardScaler fit on train only",
    "label_encoding": {"ham": 0, "spam": 1}
}
with open(os.path.join(OUT_DIR, "struct_meta.json"), "w") as f:
    json.dump(meta, f, indent=2)

print("\n✅ Structural features saved to:", OUT_DIR)
print("Files:", sorted(os.listdir(OUT_DIR)))
print("\nReady for Notebook 06 (Multimodal classifier: embeddings + structural features).")

