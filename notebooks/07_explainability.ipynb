
# ============================================
# Chapter 3 Tables Generator (CSV / Excel)
# ============================================
# Geldi Xhafaj – MSc Dissertation
# Generates T3.1 → T3.8 (+ optional T3.9) as CSV/Excel
# --------------------------------------------
import os
import json
import numpy as np
import pandas as pd
from datetime import datetime

OUT_DIR = "/content/outputs_tables"
os.makedirs(OUT_DIR, exist_ok=True)

WRITE_XLSX = True   # set False if you want only CSV
XLSX_PATH  = os.path.join(OUT_DIR, "chapter3_tables.xlsx")

# =============== 1) INPUTS (EDIT THESE) ===============

# 1.1 Dataset summary (results-relevant)
dataset_summary = {
    "Total messages": 5572,
    "Labels (mapped)": "Ham=0, Spam=1",
    "Text field": "Message",
    "Language": "English (assumed)",
    "Dedup/empty removal": "Applied"
}

# 1.2 Stratified split counts (from your Notebook 02)
splits = {
    "Train": {"Ham": 3473, "Spam": 538, "Total": 4011},
    "Val":   {"Ham": 386,  "Spam": 60,  "Total": 446},
    "Test":  {"Ham": 966,  "Spam": 149, "Total": 1115},
}

# 1.3 Tokenization & training config (used by both models)
token_config = {
    "Base model": "distilbert-base-uncased",
    "Max sequence length": "256 tokens",
    "Padding/truncation": "Enabled",
    "Optimization": "AdamW + linear LR (2e-5–5e-5)",
    "Epochs": "3 (early stopping on Val)",
    "Loss": "Class-weighted cross-entropy"
}

# 1.4 Baseline metrics (from notebook3.png)
baseline_counts = {"TN":963, "FP":3, "FN":9, "TP":140}   # Test confusion counts

baseline_metrics = {
    "Accuracy": 0.9890,
    "Spam Precision": 0.9790,
    "Spam Recall": 0.9400,
    "Spam F1": 0.9590,
    "Ham Precision": 0.9910,
    "Ham Recall": 0.9970,
    "Ham F1": 0.9940,
    "Macro-F1": 0.9770,
    "Micro-F1": 0.9890,
    # If you have baseline AUC on test, add it here:
    # "ROC-AUC": 0.9930
}

# 1.5 Structural features used (Notebook 05)
struct_feature_names = [
    "url_count","uppercase_ratio","digit_count","currency_symbol_flag",
    "punct_density","html_presence",
    # replace these placeholders with your exact 7 names in correct order:
    "struct_f7","struct_f8","struct_f9","struct_f10","struct_f11","struct_f12","struct_f13"
]

# 1.6 Multimodal metrics (from your latest run/log)
multimodal_counts = {"TN":961, "FP":5, "FN":6, "TP":143}  # Test confusion counts

multimodal_metrics = {
    "Accuracy": 0.9901,
    "Spam Precision": 0.9662,
    "Spam Recall": 0.9597,
    "Spam F1": 0.9630,
    "Ham Precision": 0.9948,         # 961/967
    "Ham Recall": 0.9950,            # 961/966
    "Ham F1": 0.9950,                # approx
    "Macro-F1": 0.9790,              # approx from per-class F1
    "Micro-F1": 0.9901,
    "ROC-AUC": 0.9944
}

# (Optional) 1.7 Objectives → Evidence map (fill later)
# Put placeholder rows; update "Evidence" with figure/table IDs once ready.
objectives_map = [
    {"Objective": "O1: Fine-tune transformer baseline", 
     "Evidence": "F3.1, T3.4", 
     "Outcome": "Strong baseline (Acc≈0.989, Macro-F1≈0.977)"},
    {"Objective": "O2: Integrate structural features (multimodal)", 
     "Evidence": "T3.6, T3.7, T3.8", 
     "Outcome": "Spam recall ↑ (+0.0197) with minimal precision trade-off"},
    {"Objective": "O3: Explainability via SHAP/LIME", 
     "Evidence": "F3.3–F3.9", 
     "Outcome": "Consistent attribution: embeddings dominate; uppercase/punct add lift"},
    {"Objective": "O4: Evaluation & trade-offs", 
     "Evidence": "F3.2–F3.4, T3.7–T3.8", 
     "Outcome": "FN ↓ (−3), TP ↑ (+3); FP low; latency μs-level"}
]

# =============== 2) HELPERS ===============
def df_from_dict(d, orient_cols=True):
    """Convert dict to DataFrame. If orient_cols=True, keys->index; else keys->columns."""
    if orient_cols:
        return pd.DataFrame({"Value": list(d.values())}, index=list(d.keys()))
    else:
        return pd.DataFrame([d])

def save_table(df: pd.DataFrame, name_stub: str):
    csv_path = os.path.join(OUT_DIR, f"{name_stub}.csv")
    df.to_csv(csv_path, index=True)
    if WRITE_XLSX:
        with pd.ExcelWriter(XLSX_PATH, engine="openpyxl", mode="a" if os.path.exists(XLSX_PATH) else "w") as xlw:
            df.to_excel(xlw, sheet_name=name_stub[:31])  # Excel sheet name max 31 chars

# =============== 3) BUILD TABLES ===============

# T3.1 – Dataset summary
t3_1 = df_from_dict(dataset_summary, orient_cols=True)
t3_1.index.name = "Attribute"
save_table(t3_1, "T3_1_dataset_summary")

# T3.2 – Stratified split counts
t3_2 = pd.DataFrame(splits).T[["Ham","Spam","Total"]]
save_table(t3_2, "T3_2_stratified_split_counts")

# T3.3 – Tokenization & training config
t3_3 = df_from_dict(token_config, orient_cols=True)
t3_3.index.name = "Setting"
save_table(t3_3, "T3_3_tokenization_training_config")

# T3.4 – Baseline DistilBERT test metrics
t3_4 = df_from_dict(baseline_metrics, orient_cols=True)
t3_4.index.name = "Metric"
save_table(t3_4, "T3_4_baseline_metrics")

# T3.5 – Structural features list
t3_5 = pd.DataFrame({"Feature": struct_feature_names, "Type": [
    "integer","float [0,1]","integer","binary","float","binary",
    "—","—","—","—","—","—","—"
]})
save_table(t3_5, "T3_5_structural_features")

# T3.6 – Multimodal test metrics
t3_6 = df_from_dict(multimodal_metrics, orient_cols=True)
t3_6.index.name = "Metric"
save_table(t3_6, "T3_6_multimodal_metrics")

# T3.7 – Baseline vs Multimodal (Δ column)
comp_keys = ["Accuracy","Spam Precision","Spam Recall","Spam F1","Ham F1","Macro-F1"]
t3_7 = pd.DataFrame({
    "Metric": comp_keys,
    "Baseline": [baseline_metrics[k] for k in comp_keys],
    "Multimodal": [multimodal_metrics[k] for k in comp_keys],
})
t3_7["Δ (Multi − Base)"] = t3_7["Multimodal"] - t3_7["Baseline"]
t3_7 = t3_7.set_index("Metric")
save_table(t3_7, "T3_7_baseline_vs_multimodal")

# T3.8 – Error counts (TN/FP/FN/TP)
t3_8 = pd.DataFrame([baseline_counts, multimodal_counts], index=["Baseline","Multimodal"])
save_table(t3_8, "T3_8_error_counts")

# (Optional) T3.9 – Objectives → Evidence map
t3_9 = pd.DataFrame(objectives_map)
save_table(t3_9, "T3_9_objectives_evidence_map")

# =============== 4) PRINT SUMMARY ===============
print("Tables generated in:", OUT_DIR)
print("CSV files:")
for f in sorted(os.listdir(OUT_DIR)):
    if f.endswith(".csv"):
        print(" -", f)
if WRITE_XLSX:
    print("\nExcel workbook:", XLSX_PATH)

